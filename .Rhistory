# check their distributions
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#original
prop.table(table(CompanyInfo$Attrition))
# install and load the C5.0 library
install.packages("C50")
library("C50")
# train the model
CompanyInfo_model = C5.0(train[-1], train$Attrition)
summary(CompanyInfo_model)
m2 = rpart(Attrition ~ ., data=CompanyInfo[1:1659,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, CompanyInfo)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model (Error)
CompanyInfo_pred = predict(CompanyInfo_model, test)
View(CompanyInfo_model)
CompanyInfo_model[["rules"]]
# reading the data set
hr = read.csv("Company.csv", header = TRUE)
# lets look at it structuer of the data
str(hr)
normalize <- function(x) { return((x - min(x)) / (max(x)-min(x)))}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
barplot(prop.table(table(hr$Attrition)),
col = rainbow(2),
ylim = c(0, 0.7),
main = "Class Distribution")
# randomize first
set.seed(1234) # sync the randomization / allows reproducibility
# create a training and a test sub-set
pd <- sample(2, nrow(hr),replace = TRUE, prob = c(0.8,0.2))
train <- hr[pd==1,]
test <-hr[pd==2,]
# data for Developing predictive Model
table(train$Attrition)
prop.table(table(train$Attrition))
table(train$Attrition)
table(test$Attrition)
# check their distributions
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#original
prop.table(table(hr$Attrition))
# install and load the C5.0 library
install.packages("C50")
install.packages("C50")
library("C50")
# train the model
hr_model = C5.0(train[-1], train$Attrition)
summary(hr_model)
m2 = rpart(Attrition ~ ., data=hr[1:1659,], method="class")
m2
install.packages("rpart")
install.packages("rpart.plot")
install.packages("rpart")
library(party)
require(rpart)
require(rpart.plot)
summary(hr_model)
# train the model
hr_model = C5.0(train[-1], train$Attrition)
library("C50")
# train the model
hr_model = C5.0(train[-1], train$Attrition)
summary(hr_model)
m2 = rpart(Attrition ~ ., data=hr[1:1659,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, hr)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model
hr_pred = predict(hr_model, test)
# train the model
hr_model = C5.0(train[,-1], train$Attrition)
summary(hr_model)
m2 = rpart(Attrition ~ ., data=hr[1:1659,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, hr)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model
hr_pred = predict(hr_model, test)
# train the model
hr_model = C5.0(train[,-2], train$Attrition)
summary(hr_model)
m2 = rpart(Attrition ~ ., data=hr[1:1659,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, hr)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model
hr_pred = predict(hr_model, test)
# test the model
hr_pred = predict(hr_model, train)
#Run CSV File
CompanyInfo <- read.csv("Company.csv", head = TRUE)
#sort the data with specific attributes
AR_CompanyInfo <- CompanyInfo[,names(CompanyInfo) %in% c('Attrition','BusinessTravel','Department',
'EducationField','Gender','JobRole','MaritalStatus')]
# write as a output file
write.csv(AR_CompanyInfo, file="AR_CompanyInfo.csv", row.names = F, na = " ")
#install.packages("arules")
library(arules)
set.seed(9999)
ar_CompanyInfo <- read.transactions("AR_CompanyInfo.csv", sep =",", rm.duplicates = TRUE)
#Run CSV File
CompanyInfo <- read.csv("Company.csv", head = TRUE)
#sort the data with specific attributes
AR_CompanyInfo <- CompanyInfo[,names(CompanyInfo) %in% c('Attrition','BusinessTravel','Department',
'EducationField','Gender','JobRole','MaritalStatus')]
# write as a output file
write.csv(AR_CompanyInfo, file="AR_CompanyInfo.csv", row.names = F, na = " ")
#install.packages("arules")
library(arules)
set.seed(9999)
ar_CompanyInfo <- read.transactions("AR_CompanyInfo.csv", sep =",", rm.duplicates = TRUE)
#summary of CompanyInfo data
summary(ar_CompanyInfo)
#display first five row
inspect(ar_CompanyInfo[1:5])
#build model bu using apriori function
apriori(ar_CompanyInfo)
AR_rules <- apriori(ar_CompanyInfo,parameter=list(support =0.01, confidence = 0.8,
minlen = 2), appearance=list(rhs=c("No", "Yes"), default="lhs"))
summary(AR_rules)
## keep tCompanyInfoee decimal places
quality(AR_rules) <- round(quality(AR_rules), digits=4)
#Rules after sorting
SortingRules <- (sort(AR_rules, by = "lift"))
## find redundant rules
subset.matrix <- is.subset(SortingRules, SortingRules, sparse = FALSE)
subset.matrix[lower.tri(subset.matrix, diag = T)] <- NA
redundant <- colSums(subset.matrix, na.rm = T) >= 1
## which rules are redundant
which(redundant)
## remove redundant rules
PruningRules <- SortingRules[!redundant]
#PruningRules
inspect(PruningRules[1:5])
#Total of rules with Attrition 'Yes' and 'No'
subset(PruningRules, items %in% "Yes")
subset(PruningRules, items %in% "No")
#install.packages("arulesViz")
#install.packages("viridisLite")
library(arulesViz)
library(viridisLite)
plot(SortingRules)
plot(PruningRules)
plot((PruningRules[1:5]), method = "graph", control = list(type="items"))
plot(PruningRules[1:20], method = "paracoord", control = list(reorder = TRUE))
plot(PruningRules, method = "grouped")
#Load Library
library("e1071")
library("caret")
library("gmodels")
#read csv file
Company<- read.csv("Company.csv")
#read csv file
CompanyNaiveBaye <- read.csv("Company.csv")
#column names
colnames(Company)
# clean data.
summary(Company)
#read csv file
CompanyNaiveBaye <- read.csv("Company.csv")
#column names
colnames(CompanyNaiveBaye)
# clean data.
summary(CompanyNaiveBaye)
str(CompanyNaiveBaye)
any(is.na(CompanyNaiveBaye))
#remove unwanted columns.
CompanyN <- CompanyNaiveBaye[ , -(c(3,6,9,10,18,20,24,26,28,31,32,34,35,36))]
#change 1st column name to Age
colnames(CompanyN)[1] <- "Age"
#write a new csv
write.csv(CompanyN, file="employeeD_new.csv", row.names = F, na = " ")
# check column names
colnames(CompanyN)
# read new csv file second time running code.
CompanyN<- read.csv("employeeD_new.csv")
#split data one for train and other for test
set.seed(2)
#takes all observation, randomly select 80% of employees and 20%, so two value.
#80% of time I want to select 1 and 20% time 2.
p <- sample(2, nrow(CompanyN), prob = c(0.8,0.2), replace =T)
train<- CompanyN[p==1,]
test<- CompanyN[p==2,]
nrow(train)
nrow(test)
#compare yes and no in two dataset
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#naive bayes
emp <- naiveBayes(Attrition ~ Age + DailyRate + Department + Education + Education + EnvironmentSatisfaction + Gender + HourlyRate + JobInvolvement + JobLevel + JobSatisfaction + MonthlyIncome + NumCompaniesWorked + OverTime + PerformanceRating + YearsInCurrentRole, data = train)
emp
prediction2<- predict(emp, test)
confusionMatrix(table(prediction2,test$Attrition))
#put tarin data in cross tables
CrossTable(prediction2,test$Attrition, prop.chisq = FALSE,prop.t = FALSE, dnn = c('predicted', 'actual'))
prediction3<- predict(emp, train)
confusionMatrix(table(prediction3,train$Attrition))
#put test data in crosstable
CrossTable(prediction3,train$Attrition, prop.chisq = FALSE,prop.t = FALSE, dnn = c('predicted', 'actual'))
#probability
prediction2<- predict(emp, test, type="raw")
prediction2
prediction2
prediction3
summary(CompanyNaiveBaye$ï..Age)
summarise_layout(CompanyNaiveBaye$ï..Age)
avg.age <- df %>% select(Gender, Age) %>% group_by(Gender) %>% summarize(avg=mean(Age))
avg.age
# reading the Company.csv data set
CompanyDT = read.csv("Company.csv", header = TRUE)
# lets look at it structure of the dataset from IBM
str(CompanyDT)
normalize <- function(x) { return((x - min(x)) / (max(x)-min(x)))}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
barplot(prop.table(table(CompanyDT$Attrition)),
col = rainbow(2),
ylim = c(0, 0.7),
main = "Class Distribution")
# randomize first
set.seed(6969) # sync the randomization / allows reproducibility
# create a training and a test sub-set
pd <- sample(2, nrow(CompanyDT),replace = TRUE, prob = c(0.8,0.2))
train <- CompanyDT[pd==1,]
test <-CompanyDT[pd==2,]
# data for Developing predictive Model
table(train$Attrition)
prop.table(table(train$Attrition))
table(train$Attrition)
table(test$Attrition)
# check their distributions
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#original
prop.table(table(CompanyDT$Attrition))
# install and load the C5.0 library
install.packages("C50")
install.packages("C50")
library("C50")
# train the model
CompanyDT_model = C5.0(train[-1], train$Attrition)
# train the model
CompanyDT_model = C5.0(train[,-1], train$Attrition)
View(CompanyDT_model)
summary(CompanyDT_model)
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1659,], method="class")
install.packages("party")
library(party)
require(rpart)
require(rpart.plot)
install.packages("party")
library(party)
require(rpart)
require(rpart.plot)
install.packages("party")
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1470,], method="class")
summary(CompanyDT_model)
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1470,], method="class")
m2
install.packages("rpart")
install.packages("rpart.plot")
install.packages("party")
library(party)
require(rpart)
require(rpart.plot)
# reading the Company.csv data set
CompanyDT = read.csv("Company.csv", header = TRUE)
# lets look at it structure of the dataset from IBM
str(CompanyDT)
normalize <- function(x) { return((x - min(x)) / (max(x)-min(x)))}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
barplot(prop.table(table(CompanyDT$Attrition)),
col = rainbow(2),
ylim = c(0, 0.7),
main = "Class Distribution")
# randomize first
set.seed(6969) # sync the randomization / allows reproducibility
# create a training and a test sub-set
pd <- sample(2, nrow(CompanyDT),replace = TRUE, prob = c(0.8,0.2))
train <- CompanyDT[pd==1,]
test <-CompanyDT[pd==2,]
# data for Developing predictive Model
table(train$Attrition)
prop.table(table(train$Attrition))
table(train$Attrition)
table(test$Attrition)
# check their distributions
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#original
prop.table(table(CompanyDT$Attrition))
# install and load the C5.0 library
install.packages("C50")
library("C50")
# train the model
CompanyDT_model = C5.0(train[,-1], train$Attrition)
summary(CompanyDT_model)
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1470,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, CompanyDT)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model (Error)
CompanyDT_pred = predict(CompanyDT_model, test)
# Classification Models
library(randomForest)
# ntree=Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.
# mtry=Number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)
# importance=Should importance of predictors be assessed?
# Replace=Should sampling of cases be done with or without replacement?
# control=a list of options that control details of the rpart algorithm.
# use surrogate=how to use surrogates in the splitting process. 0 means display only; an observation with a missing value for the primary split rule is not sent further down the tree. other 1,2
# maxsurrogate=the number of surrogate splits retained in the output. If this is set to zero the compute time will be reduced, since approximately half of the computational time (other than setup) is used in the search for surrogate splits.
model_RF <- randomForest(Attrition ~ .,
data=TrainingData,
ntree=500,
mtry=2,
importance=TRUE,
replace=FALSE)
# ntree=Number of trees to grow. This should not be set to too small a number, to ensure that every input row gets predicted at least a few times.
# mtry=Number of variables randomly sampled as candidates at each split. Note that the default values are different for classification (sqrt(p) where p is number of variables in x) and regression (p/3)
# importance=Should importance of predictors be assessed?
# Replace=Should sampling of cases be done with or without replacement?
# control=a list of options that control details of the rpart algorithm.
# use surrogate=how to use surrogates in the splitting process. 0 means display only; an observation with a missing value for the primary split rule is not sent further down the tree. other 1,2
# maxsurrogate=the number of surrogate splits retained in the output. If this is set to zero the compute time will be reduced, since approximately half of the computational time (other than setup) is used in the search for surrogate splits.
model_RF <- randomForest(Attrition ~ .,
data=CompanyDT,
ntree=500,
mtry=2,
importance=TRUE,
replace=FALSE)
# Generate a textual view of the Random Forest
model_RF
summary(model_RF)
plot(model_RF, main="")
legend("topright", c("OOB", "0", "1"), text.col=1:6, lty=1:3, col=1:3)
title(main="Error Rates Random Forest")
#testing the model on test data
pred_Test_RF<-predict(model_RF,ValidationData[1:6], type="class")
#read the desired file
CompanyIBM <-read.csv("Company.csv")
#str f() will provide the meta data of the dataset it will describe the data type and overview of the data set
str(CompanyIBM)
# summary f() provide the insight of the data set mainly it will let us know about the categorical(factors) data
# and central tendency like (mean,median quartiles) for numerical data.
summary(CompanyIBM)
#It is possible that not all variables are correlated with the label, feature selection is therefore performed to filter out the most relevant ones.
#As the data set is a mix of both numerical and categorical variables,
# A good way to select feature is by training a model and then rank the variable importance so as to select the most salient ones.
# Here I am using decision tree model which come under rpart library to rank important variable.
library(rpart)
dt<-rpart( Attrition~.,data=CompanyIBM,control=rpart.control(minsplit = 10))
dt$variable.importance
library(caret)
#other way of doing it by Droping the the columns with no variability.
drop_var<-names(CompanyIBM[, nearZeroVar(CompanyIBM)])
drop_var #it will show the variable names with zero variability
#Drop Over18 as there is no variability, all are Y.
#Drop EmployeeCount as there is no variability, all are 1.
#Drop StandardHours as there is no variability, all are 80.
#Drop EmployeeNumber becouse they are just assigned numers to each Employee
CompanyIBM$Over18 <- NULL
CompanyIBM$EmployeeCount <- NULL
CompanyIBM$StandardHours <- NULL
CompanyIBM$EmployeeNumber<-NULL
# convert certain integer variable to factor variable  as they make more sense as factor data type
conv_fact<- c("Education", "EnvironmentSatisfaction", "JobInvolvement", "JobLevel", "JobSatisfaction", "PerformanceRating", "RelationshipSatisfaction", "StockOptionLevel","TrainingTimesLastYear","WorkLifeBalance")
CompanyIBM[, conv_fact] <- lapply((CompanyIBM[, conv_fact]), as.factor)
#number of attritions from data
table(CompanyIBM$Attrition)
ggplot(CompanyIBM, aes(x=Attrition, fill=Attrition)) + geom_bar()
table(CompanyIBM$OverTime, CompanyIBM$Attrition)
ggplot(CompanyIBM, aes(OverTime, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
summary(CompanyIBM$MonthlyIncome)
MnthlyIncome <- cut(CompanyIBM$MonthlyIncome, 10, include.lowest = TRUE, labels=c(1,2,3,4,5,6,7,8,9,10))
ggplot(CompanyIBM, aes(MnthlyIncome, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
summary(CompanyIBM$TotalWorkingYears)
TtlWkgYrs <- cut(CompanyIBM$TotalWorkingYears, 10, include.lowest = TRUE)
ggplot(CompanyIBM, aes(TtlWkgYrs, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
summary(CompanyIBM$HourlyRate)
HrlyRate<- cut(CompanyIBM$HourlyRate, 7, include.lowest = TRUE)
ggplot(CompanyIBM, aes(HrlyRate, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
table(CompanyIBM$JobRole, CompanyIBM$Attrition)
ggplot(CompanyIBM, aes(JobRole, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
summary(CompanyIBM$Age)
A_g_e <- cut(CompanyIBM$Age, 8, include.lowest = TRUE)
ggplot(CompanyIBM, aes(A_g_e, ..count.., fill = factor(Attrition))) + geom_bar(position="dodge")
#Create Training and Testing Sets.
library(dplyr)
ModelData<-sample_frac(CompanyIBM, 0.75)
sid<-as.numeric(rownames(ModelData)) # because rownames() returns character
ValidateData<-CompanyIBM[-sid,]
#As per the rpart model, Overtime, MonthlyIncome, TotalWorkingYears, HourlyRate, JobRole and Age are the most important factors influencing the attrition rates. Let's explore these variables.
#only taking the variables of interest to speed up the process and reduce the complexity you can add any variable if you think it may affect on model
Subject <- c("OverTime","MonthlyIncome","TotalWorkingYears","HourlyRate","JobRole","Age")
Objective  <- "Attrition"
TrainingData<-ModelData[c(Subject, Objective)]
ValidationData<-ValidateData[c(Subject, Objective)]
library(ROCR)
### AUC plot function
fun.aucplot <- function(pred, obs, title){
# Run the AUC calculations
ROC_perf <- performance(prediction(pred,obs),"tpr","fpr")
ROC_sens <- performance(prediction(pred,obs),"sens","spec")
ROC_auc <- performance(prediction(pred,obs),"auc")
# Spawn a new plot window (Windows OS)
graphics.off(); x11(h=6,w=6)
# Plot the curve
plot(ROC_perf,colorize=T,print.cutoffs.at=seq(0,1,by=0.1),lwd=3,las=1,main=title)
abline(a=0,b=1)
# Add some statistics to the plot
text(1,0.15,labels=paste("AUC = ",round(ROC_auc@y.values[[1]],digits=2),sep=""),adj=1)
}
#============================================================
# Classification Models
# Support vector machine.
library(e1071)
model <- svm(Attrition ~ .,data=TrainingData)#SVM model on data set
summary(model)
predict_val <- predict(model,ValidationData[1:6]) #predicting the values
library(caret)
#confusionMatrix Calculates a cross-tabulation of observed and predicted classes with associated statistics.
result_Test <- confusionMatrix(predict_val,ValidationData$Attrition)
install.packages("rpart")
install.packages("rpart")
install.packages("rpart.plot")
install.packages("party")
library(party)
install.packages("party")
library("party", lib.loc="~/R/win-library/3.5")
library("rpart", lib.loc="C:/Program Files/R/R-3.5.3/library")
library("rpart.plot", lib.loc="~/R/win-library/3.5")
library("rpart", lib.loc="~/R/win-library/3.5")
# reading the Company.csv data set
CompanyDT = read.csv("Company.csv", header = TRUE)
# lets look at it structure of the dataset from IBM
str(CompanyDT)
normalize <- function(x) { return((x - min(x)) / (max(x)-min(x)))}
normalize(c(1,2,3,4,5))
normalize(c(10,20,30,40,50))
barplot(prop.table(table(CompanyDT$Attrition)),
col = rainbow(2),
ylim = c(0, 0.7),
main = "Class Distribution")
# randomize first
set.seed(6969) # sync the randomization / allows reproducibility
# create a training and a test sub-set
pd <- sample(2, nrow(CompanyDT),replace = TRUE, prob = c(0.8,0.2))
train <- CompanyDT[pd==1,]
test <-CompanyDT[pd==2,]
# data for Developing predictive Model
table(train$Attrition)
prop.table(table(train$Attrition))
table(train$Attrition)
table(test$Attrition)
# check their distributions
prop.table(table(train$Attrition))
prop.table(table(test$Attrition))
#original
prop.table(table(CompanyDT$Attrition))
# install and load the C5.0 library
install.packages("C50")
library("C50", lib.loc="~/R/win-library/3.5")
library("C50")
# train the model
CompanyDT_model = C5.0(train[,-1], train$Attrition)
summary(CompanyDT_model)
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1470,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, CompanyDT)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model (Error)
CompanyDT_pred = predict(CompanyDT_model, test)
# train the model
CompanyDT_model = C5.0(train[,-2], train$Attrition)
summary(CompanyDT_model)
m2 = rpart(Attrition ~ ., data=CompanyDT[1:1470,], method="class")
m2
# Decision with rpat
m2 <- rpart(Attrition~DailyRate+DistanceFromHome+JobInvolvement+MonthlyRate+RelationshipSatisfaction+WorkLifeBalance
+ TrainingTimesLastYear+RelationshipSatisfaction +EnvironmentSatisfaction+HourlyRate, CompanyDT)
library(rpart.plot)
rpart.plot(m2,extra = 4)
# test the model (Error)
CompanyDT_pred = predict(CompanyDT_model, test)
View(CompanyDT)
is.na()
is.na(CompanyDT)
